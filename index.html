---
layout: default
nav_active: index
title: Touché @ CLEF
description: Touché @ CLEF Argument Retrieval
---


<main class="uk-section">
        <div class="uk-container">
                <h2 class="uk-lead">Task</h2>
                <p class=" uk-text-left">
                        Decision making processes, be it at the societal or at the personal level, eventually come to a point where one side will challenge the other with a why-question, which is a prompt to justify one’s stance.
                        Thus, technologies for argument mining and argumentation processing are maturing at a rapid
                        pace, giving rise for the first time to argument retrieval. We invite to participate in the first lab on <strong>Argument Retrieval</strong> at CLEF 2020 featuring two subtasks: <br> <br>(1) retrieval in a focused argument collection to support argumentative conversations; <br> (2) retrieval in a generic web crawl to answer comparative questions with argumentative results. <br> <br>
                        The <strong>(1) subtask</strong> is motivated by the support of users who search for arguments directly, e.g., by supporting their stance, and targets argumentative conversations. The task is to retrieve arguments from the provided dataset of the focused crawl with content from online debate portals for the 50 given topics, covering a wide range of controversial issues. <br> <br>
                        The <strong>(2) subtask</strong> is motivated by the support of users with arguments in personal decisions from everyday life where it comes to making choices. The task is to retrieve ranked documents from a general web crawl ClueWeb12 that help the users to answer their comparative question. We provide 50 such questions. <br> <br>
                        Should you have questions contact us via touche@webis.de.<br>
                </p>
                <!-- <p>To register for either task: <a href="http://clef2020-labs-registration.dei.unipd.it/registrationForm.php">Register Now</a>.</p> -->
                <a class="uk-button uk-button-primary" href="http://clef2020-labs-registration.dei.unipd.it/registrationForm.php">Register now</a>
                <h2 class="uk-lead">Dates</h2>
                <p class="uk-text-left">
                        <strong>November 22, 2019:</strong> Training data available, competition begins.&nbsp; <br>
                        <strong>February 01, 2020:</strong> Submission system opens.<br>
                        <!--<a
                                href="#"
                                class="uk-text-bold" style="text-decoration:underline">Leader board (TIRA)</a><br>-->
                        <strong>April 26, 2020:</strong> Submission system closed, manual evaluation.
                        begins.<br>
                        <strong>TBA :</strong> Leader board (TIRA).<br>
                </p>
                <p>The timezone of all deadlines is <a href="https://en.wikipedia.org/wiki/Anywhere_on_Earth">Anywhere on Earth</a>.</p>
                <h2 class="uk-lead">Data</h2>
                <p class="uk-text-left">
                        Argument topics for subtask (1) and comparative questions for subtask (2) will be send to each team via email upon completed registration. The topics will be provided as XML files.<br><br>
                        Example topic for <strong>subtask (1):</strong><br><br>
                        &nbsp;&nbsp;&nbsp;&#60;topic&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;num&#62;1&#60;/num&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;title&#62;Is climate change real?&#60;/title&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;description&#62;You read an opinion piece on how climate change is a hoax
                            and disagree. Now you are looking for arguments supporting
                            the claim that climate change is in fact real.&#60;/description&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;narrative&#62;Relevant arguments will support the given stance that
                            climate change is real or attack a hoax side's argument.&#60;/narrative&#62;<br>
                        &nbsp;&nbsp;&nbsp;&#60;/topic&#62;<br><br>

                        <strong>Document collections.</strong> To search for relevant arguments, you can use your own index based on the dataset <a href="https://webis.de/data/args-me.html">args-me</a> or for simplicity deploy an API of the search engine <a href="https://www.args.me/api-en.html">args.me</a>.<br><br>

                        Example topic for <strong>subtask (2):</strong><br><br>
                        &nbsp;&nbsp;&nbsp;&#60;topic&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;num&#62;1&#60;/num&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;title&#62;What are advantages and disadvantages of PHP over Python and vice versa?&#60;/title&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;description&#62;The user is looking for differences and similarities of PHP and Python and wants to know about scenarios that favor one over the other.&#60;/description&#62;<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&#60;narrative&#62;Relevant documents may contain an overview of more than these two programming languages but must include both of them with an explicit comparison of these two.&#60;/narrative&#62;<br>
                        &nbsp;&nbsp;&nbsp;&#60;/topic&#62;<br><br>

                <strong>Document collections.</strong> To search for relevant documents, you can use your own index based on <a href="https://lemurproject.org/clueweb12/">ClueWeb12</a> or for simplicity deploy an API of the search engine <a href="https://www.chatnoir.eu/doc/">ChatNoir</a>. You will recieve credentials to access API upon completed registration.
                </p>

                <h2 class="uk-lead">Evaluation</h2>
                <p class="uk-text-left">
                        We will add detailed evaluation requirements and submission instructions next week.
                        <!-- 
                        To provide fast approximations of model performance, the public leaderboard
                        will be
                        updated with <a href="https://github.com/tagucci/pythonrouge" style="text-decoration:underline"
                                class="uk-text-bold">ROUGE</a> scores.
                        You will be able to self-evaluate your software using the <a href="http://tira.io/" target="new"
                                style="text-decoration:underline" class="uk-text-bold">TIRA</a>
                        service. You can find the user guide <a href="https://www.tira.io/static/tira-vm-user-guide.pdf"
                                style="text-decoration:underline" target="new" class="uk-text-bold">here</a>. &nbsp; Additionally, a qualitative evaluation will be performed through crowdsourcing. Human
                                annotators will rate each candidate summary according to five linguistic qualities as suggested
                                by the <a href="https://duc.nist.gov/pubs/2006papers/duc2006.pdf" target="new"
                                        style="text-decoration:underline" class="uk-text-bold">DUC
                                        guidelines</a>. -->
                </p>
                <h2 class="uk-lead">Runs Submission</h2>
                <p class="uk-text-left">
                        We encourage participants to use <a href="https://www.tira.io/">TIRA</a> for their submissions to increase replicability of the experiments. We provide a <a href="/tira-guide.html">dedicated TIRA tutorial for Touché</a> and are available to walk you through. You can also submit runs per email. In both cases, we will review your submission promptly and provide feedback.
                <p class="uk-text-left">
                        Runs may be either automatic or manual. An automatic run is made without any manual manipulation of the given topic titles. Your run is automatic if you do not use description and narrative for developing approaches. A manual run is anything that is not an automatic run. Please let us know which of your runs are manual upon submission.
                </p>
                <p class="uk-text-left">
                        The submission format for both tasks will follow the standard TREC format:<br><br>
                        <code>qid Q0 doc rank score tag</code><br><br>

                        With:
                        <ul>
                                <li><code>qid</code>: The topic number.</li>
                                <li><code>Q0</code>: Unused, should always be Q0.</li>
                                <li>
                                       <code>doc</code>: The document id returned by your system for the topic <code>qid</code>:
                                       <ul>
                                                <li>For <strong>subtask (1)</strong>: Use the official args-me id.</li>
                                                <li>For <strong>subtask (2)</strong>: Use the official ClueWeb12 id.</li>
                                       </ul>
                                </li>
                                <li><code>rank</code>: The rank the document is retrieved at.</li>
                                <li><code>score</code>: The score (integer or floating point) that generated the ranking. The score must be in descending (non-increasing) order. It is important to handle tied scores. (trec_eval sorts documents by the score values and not your rank values.)</li>
                                <li><code>tag</code>: A tag that identifies your group and the method you used to produce the run.</li>
                        </ul>

                                The fields should be spectated with a whitespace. The width of the columns in the format is not important, but it is important to include all columns and have some amount of white space between the columns.<br><br>

                                An example run for task 1 is:<br>

                                <code>1 Q0 10113b57-2019-04-18T17:05:08Z-00001-000 1 17.89 myGroupMyMethod</code><br>
                                <code>1 Q0 100531be-2019-04-18T19:18:31Z-00000-000 2 16.43 myGroupMyMethod</code><br>
                                <code>1 Q0 10006689-2019-04-18T18:27:51Z-00000-000 3 16.42 myGroupMyMethod</code><br>
                                <code>...</code><br><br>

                                An example run for task 2 is:<br>

                                <code>1 Q0 clueweb09-en0010-85-29836 1 17.89 myGroupMyMethod</code><br>
                                <code>1 Q0 clueweb09-en0010-86-00457 2 16.43 myGroupMyMethod</code><br>
                                <code>1 Q0 clueweb09-en0010-86-09202 3 16.42 myGroupMyMethod</code><br>
                                <code>...</code><br>
                </p>
                <h2 id="index-program">Program</h2>
                        The workshop program will be announced closer to the conference.

            <h2 id="index-organizing-committee">Organizing Committee</h2>
            <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
                {% include people-cards/bondarenko.html %}
                {% include people-cards/hagen.html %}
                {% include people-cards/potthast.html %}
                {% include people-cards/wachsmuth.html %}
                {% include people-cards/beloucif.html %}
                {% include people-cards/bieman.html %}
                {% include people-cards/froebe.html %}
                {% include people-cards/panchenko.html %}
                {% include people-cards/stein.html %}
            </div>
            <div class="uk-container uk-padding-large uk-padding-remove-bottom">
                {% include organizations/clef-organizations-section.html year=2020 %}
            </div>
        </div>
</main>
