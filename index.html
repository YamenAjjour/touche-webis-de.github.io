---
layout: default
nav_active: index
title: Touché @ CLEF
description: Touché @ CLEF Argument Retrieval
---


<main class="uk-section">
        <div class="uk-container">
                <h1 class="uk-lead">Touché @ CLEF Argument Retrieval</h1>
                <h2 class="uk-lead">Task</h2>
                <p class=" uk-text-left">
                        Decision making processes, be it at the societal or at the personal level, eventually come to a point where one side will challenge the other with a why-question, which is a prompt to justify one’s stance.
                        Thus, technologies for argument mining and argumentation processing are maturing at a rapid
                        pace, giving rise for the first time to argument retrieval. We organize the first lab on <strong>Argument Retrieval</strong> at CLEF 2020 featuring two subtasks: <br> (1) retrieval in a focused argument collection to support argumentative conversations; <br> (2) retrieval in a generic web crawl to answer comparative questions with argumentative results. <br>
                </p>
                <h2 class="uk-lead">Dates</h2>
                <p class="uk-text-left">
                        <strong>TBA :</strong> Training data available, competition begins.&nbsp; <br>
                        <strong>TBA :</strong> Submission system open.<br>
                        <strong>TBA :</strong> Leader board (TIRA).<br><!--<a
                                href="#"
                                class="uk-text-bold" style="text-decoration:underline">Leader board (TIRA)</a><br>-->
                        <strong>TBA :</strong> Submission system closed (23:59 PM UTC), manual evaluation
                        begins.<br>
                </p>
                <h2 class="uk-lead">Data</h2>
                <p class="uk-text-left">Will be added soon. <br></p>

                <h2 class="uk-lead">Evaluation</h2>
                <p class="uk-text-left">
                        Will be added soon.
                        <!-- 
                        To provide fast approximations of model performance, the public leaderboard
                        will be
                        updated with <a href="https://github.com/tagucci/pythonrouge" style="text-decoration:underline"
                                class="uk-text-bold">ROUGE</a> scores.
                        You will be able to self-evaluate your software using the <a href="http://tira.io/" target="new"
                                style="text-decoration:underline" class="uk-text-bold">TIRA</a>
                        service. You can find the user guide <a href="https://www.tira.io/static/tira-vm-user-guide.pdf"
                                style="text-decoration:underline" target="new" class="uk-text-bold">here</a>. &nbsp; Additionally, a qualitative evaluation will be performed through crowdsourcing. Human
                                annotators will rate each candidate summary according to five linguistic qualities as suggested
                                by the <a href="https://duc.nist.gov/pubs/2006papers/duc2006.pdf" target="new"
                                        style="text-decoration:underline" class="uk-text-bold">DUC
                                        guidelines</a>. -->
                </p>
        </div>
</main>